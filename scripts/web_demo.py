# å¯¼å…¥å¿…è¦çš„åº“
import random  # ç”¨äºç”Ÿæˆéšæœºæ•°
import re  # æ­£åˆ™è¡¨è¾¾å¼å¤„ç†
import time  # æ—¶é—´ç›¸å…³æ“ä½œ
import numpy as np  # æ•°å€¼è®¡ç®—åº“
import streamlit as st  # ç½‘é¡µåº”ç”¨æ¡†æ¶
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶

# é…ç½®Streamlité¡µé¢
st.set_page_config(
    page_title="MiniMind",  # é¡µé¢æ ‡é¢˜
    initial_sidebar_state="collapsed"  # åˆå§‹ä¾§è¾¹æ çŠ¶æ€ï¼ˆæŠ˜å ï¼‰
)

# è‡ªå®šä¹‰CSSæ ·å¼ï¼Œç”¨äºç¾åŒ–ç•Œé¢å…ƒç´ 
st.markdown("""
    <style>
        /* æ“ä½œæŒ‰é’®æ ·å¼ */
        .stButton button {
            border-radius: 50% !important;  /* åœ†å½¢æŒ‰é’® */
            width: 32px !important;         /* å›ºå®šå®½åº¦ */
            height: 32px !important;        /* å›ºå®šé«˜åº¦ */
            padding: 0 !important;          /* ç§»é™¤å†…è¾¹è· */
            background-color: transparent !important;  /* é€æ˜èƒŒæ™¯ */
            border: 1px solid #ddd !important;  /* è¾¹æ¡†æ ·å¼ */
            display: flex !important;       /* å¼¹æ€§å¸ƒå±€ */
            align-items: center !important;  /* å‚ç›´å±…ä¸­ */
            justify-content: center !important;  /* æ°´å¹³å±…ä¸­ */
            font-size: 14px !important;      /* å­—ä½“å¤§å° */
            color: #666 !important;         /* å­—ä½“é¢œè‰² */
            margin: 5px 10px 5px 0 !important;  /* å¤–è¾¹è· */
        }
        /* æŒ‰é’®æ‚¬åœæ•ˆæœ */
        .stButton button:hover {
            border-color: #999 !important;   /* æ‚¬åœè¾¹æ¡†é¢œè‰² */
            color: #333 !important;          /* æ‚¬åœå­—ä½“é¢œè‰² */
            background-color: #f5f5f5 !important;  /* æ‚¬åœèƒŒæ™¯è‰² */
        }
        /* ä¸»å†…å®¹åŒºä¸Šè¾¹è·è°ƒæ•´ */
        .stMainBlockContainer > div:first-child {
            margin-top: -50px !important;
        }
        /* åº•éƒ¨è¾¹è·è°ƒæ•´ */
        .stApp > div:last-child {
            margin-bottom: -35px !important;
        }
        /* é‡ç½®æŒ‰é’®åŸºç¡€æ ·å¼ */
        .stButton > button {
            all: unset !important;           /* é‡ç½®æ‰€æœ‰é»˜è®¤æ ·å¼ */
            box-sizing: border-box !important;  /* ç›’æ¨¡å‹è®¾ç½® */
            border-radius: 50% !important;   /* åœ†å½¢æŒ‰é’® */
            width: 18px !important;          /* æ›´å°çš„å°ºå¯¸ */
            height: 18px !important;
            min-width: 18px !important;
            min-height: 18px !important;
            max-width: 18px !important;
            max-height: 18px !important;
            padding: 0 !important;
            background-color: transparent !important;
            border: 1px solid #ddd !important;
            display: flex !important;
            align-items: center !important;
            justify-content: center !important;
            font-size: 14px !important;
            color: #888 !important;         /* æ›´æµ…çš„å­—ä½“é¢œè‰² */
            cursor: pointer !important;      /* é¼ æ ‡æŒ‡é’ˆæ ·å¼ */
            transition: all 0.2s ease !important;  /* è¿‡æ¸¡åŠ¨ç”» */
            margin: 0 2px !important;        /* å¤–è¾¹è·è°ƒæ•´ */
        }
    </style>
""", unsafe_allow_html=True)  # å…è®¸ä¸å®‰å…¨HTML

# å…¨å±€å˜é‡åˆå§‹åŒ–
system_prompt = []  # ç³»ç»Ÿæç¤ºè¯­åˆ—è¡¨
device = "cuda" if torch.cuda.is_available() else "cpu"  # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ç±»å‹

def process_assistant_content(content):
    """
    å¤„ç†åŠ©æ‰‹è¿”å›å†…å®¹ä¸­çš„ç‰¹æ®Šæ ‡è®°ï¼Œè½¬æ¢ä¸ºHTMLå…ƒç´ 
    å‚æ•°:
        content (str): åŸå§‹åŠ©æ‰‹è¿”å›å†…å®¹
    è¿”å›:
        str: å¤„ç†åçš„HTMLå†…å®¹
    """
    # ä»…å¯¹R1ç‰ˆæœ¬æ¨¡å‹è¿›è¡Œå¤„ç†
    if 'R1' not in MODEL_PATHS[selected_model][1]:
        return content

    # å¤„ç†å®Œæ•´åŒ…å«<think>æ ‡ç­¾çš„æƒ…å†µ
    if '<think>' in content and '</think>' in content:
        content = re.sub(
            r'(<think>)(.*?)(</think>)',
            r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\2</details>',
            content,
            flags=re.DOTALL
        )

    # å¤„ç†åªæœ‰å¼€å§‹æ ‡ç­¾çš„æƒ…å†µ
    if '<think>' in content and '</think>' not in content:
        content = re.sub(
            r'<think>(.*?)$',
            r'<details open style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†ä¸­...</summary>\1</details>',
            content,
            flags=re.DOTALL
        )

    # å¤„ç†åªæœ‰ç»“æŸæ ‡ç­¾çš„æƒ…å†µ
    if '<think>' not in content and '</think>' in content:
        content = re.sub(
            r'(.*?)</think>',
            r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\1</details>',
            content,
            flags=re.DOTALL
        )

    return content

@st.cache_resource  # Streamlitç¼“å­˜è£…é¥°å™¨ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
def load_model_tokenizer(model_path):
    """
    åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
    å‚æ•°:
        model_path (str): æ¨¡å‹æœ¬åœ°è·¯å¾„
    è¿”å›:
        tuple: (æ¨¡å‹å¯¹è±¡, åˆ†è¯å™¨å¯¹è±¡)
    """
    from transformers import AutoModelForCausalLM, AutoTokenizer
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True  # ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆè‡ªå®šä¹‰æ¨¡å‹éœ€è¦ï¼‰
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,  # ä½¿ç”¨å®Œæ•´çš„åˆ†è¯å™¨åŠŸèƒ½
        trust_remote_code=True
    )
    model = model.eval().to(device)  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    return model, tokenizer

def clear_chat_messages():
    """æ¸…ç©ºèŠå¤©ä¼šè¯å†å²"""
    del st.session_state.messages
    del st.session_state.chat_messages

def init_chat_messages():
    """åˆå§‹åŒ–èŠå¤©æ¶ˆæ¯ï¼Œå¤„ç†å†å²æ¶ˆæ¯æ˜¾ç¤º"""
    if "messages" in st.session_state:
        for i, message in enumerate(st.session_state.messages):
            if message["role"] == "assistant":
                # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯ï¼Œå¸¦åˆ é™¤æŒ‰é’®
                with st.chat_message("assistant", avatar=image_url):
                    st.markdown(process_assistant_content(message["content"]), unsafe_allow_html=True)
                    # ä¸ºæ¯æ¡æ¶ˆæ¯æ·»åŠ åˆ é™¤æŒ‰é’®
                    if st.button("ğŸ—‘", key=f"delete_{i}"):
                        # åˆ é™¤å¯¹åº”çš„ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
                        st.session_state.messages.pop(i)
                        st.session_state.messages.pop(i - 1)
                        st.session_state.chat_messages.pop(i)
                        st.session_state.chat_messages.pop(i - 1)
                        st.rerun()  # ç«‹å³åˆ·æ–°ç•Œé¢
            else:
                # ç”¨æˆ·æ¶ˆæ¯å³å¯¹é½æ˜¾ç¤º
                st.markdown(
                    f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: #ddd; border-radius: 10px; color: black;">{message["content"]}</div></div>',
                    unsafe_allow_html=True)
    else:
        # åˆå§‹åŒ–ç©ºçš„æ¶ˆæ¯åˆ—è¡¨
        st.session_state.messages = []
        st.session_state.chat_messages = []
    return st.session_state.messages

# è¾…åŠ©åŠŸèƒ½å‡½æ•°
def regenerate_answer(index):
    """é‡æ–°ç”ŸæˆæŒ‡å®šä½ç½®çš„å›ç­”"""
    st.session_state.messages.pop()
    st.session_state.chat_messages.pop()
    st.rerun()

def delete_conversation(index):
    """åˆ é™¤æŒ‡å®šä½ç½®çš„å¯¹è¯"""
    st.session_state.messages.pop(index)
    st.session_state.messages.pop(index - 1)
    st.session_state.chat_messages.pop(index)
    st.session_state.chat_messages.pop(index - 1)
    st.rerun()

# ä¾§è¾¹æ é…ç½®åŒºåŸŸ
st.sidebar.title("æ¨¡å‹è®¾å®šè°ƒæ•´")
st.sidebar.text("ã€æ³¨ã€‘è®­ç»ƒæ•°æ®åå·®ï¼Œå¢åŠ ä¸Šä¸‹æ–‡è®°å¿†æ—¶\nå¤šè½®å¯¹è¯ï¼ˆè¾ƒå•è½®ï¼‰å®¹æ˜“å‡ºç°èƒ½åŠ›è¡°å‡")

# å¯¹è¯å†å²æ•°é‡æ»‘å—ï¼ˆæ­¥é•¿2ï¼‰
st.session_state.history_chat_num = st.sidebar.slider(
    "Number of Historical Dialogues", 
    0, 6, 0, step=2
)

# ç”Ÿæˆå‚æ•°é…ç½®
st.session_state.max_new_tokens = st.sidebar.slider(
    "Max Sequence Length", 
    256, 8192, 8192, step=1
)
st.session_state.top_p = st.sidebar.slider(
    "Top-P", 
    0.8, 0.99, 0.85, step=0.01
)
st.session_state.temperature = st.sidebar.slider(
    "Temperature", 
    0.6, 1.2, 0.85, step=0.01
)

# æ¨¡å‹è·¯å¾„æ˜ å°„å­—å…¸
MODEL_PATHS = {
    "MiniMind2-R1 (0.1B)": ["../MiniMind2-R1", "MiniMind2-R1"],
    "MiniMind2-Small-R1 (0.02B)": ["../MiniMind2-Small-R1", "MiniMind2-Small-R1"],
    "MiniMind2 (0.1B)": ["../MiniMind2", "MiniMind2"],
    "MiniMind2-MoE (0.15B)": ["../MiniMind2-MoE", "MiniMind2-MoE"],
    "MiniMind2-Small (0.02B)": ["../MiniMind2-Small", "MiniMind2-Small"],
    "MiniMind-V1 (0.1B)": ["../minimind-v1", "MiniMind-V1"],
    "MiniMind-V1-MoE (0.1B)": ["../minimind-v1-moe", "MiniMind-V1-MoE"],
    "MiniMind-V1-Small (0.02B)": ["../minimind-v1-small", "MiniMind-V1-Small"],
}

# æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†ï¼ˆé»˜è®¤é€‰ä¸­ç¬¬ä¸‰ä¸ªé€‰é¡¹ï¼‰
selected_model = st.sidebar.selectbox(
    'Models', 
    list(MODEL_PATHS.keys()), 
    index=2
)
model_path = MODEL_PATHS[selected_model][0]  # è·å–é€‰æ‹©çš„æ¨¡å‹è·¯å¾„

# é¡µé¢å¤´éƒ¨åŒºåŸŸ
image_url = "https://www.modelscope.cn/api/v1/studio/gongjy/MiniMind/repo?Revision=master&FilePath=images%2Flogo2.png&View=true"
slogan = f"Hi, I'm {MODEL_PATHS[selected_model][1]}"  # åŠ¨æ€ç”Ÿæˆæ ‡è¯­

# ä½¿ç”¨HTMLæ„å»ºé¡µé¢å¤´éƒ¨å¸ƒå±€
st.markdown(
    f'<div style="display: flex; flex-direction: column; align-items: center; text-align: center; margin: 0; padding: 0;">'
    '<div style="font-style: italic; font-weight: 900; margin: 0; padding-top: 4px; display: flex; align-items: center; justify-content: center; flex-wrap: wrap; width: 100%;">'
    f'<img src="{image_url}" style="width: 45px; height: 45px; "> '  # å“ç‰ŒLogo
    f'<span style="font-size: 26px; margin-left: 10px;">{slogan}</span>'  # åŠ¨æ€æ ‡è¯­
    '</div>'
    '<span style="color: #bbb; font-style: italic; margin-top: 6px; margin-bottom: 10px;">å†…å®¹å®Œå…¨ç”±AIç”Ÿæˆï¼Œè¯·åŠ¡å¿…ä»”ç»†ç”„åˆ«<br>Content AI-generated, please discern with care</span>'
    '</div>',
    unsafe_allow_html=True
)

def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­ä¿è¯ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True  # ç¡®ä¿å·ç§¯æ“ä½œç»“æœç¡®å®š
    torch.backends.cudnn.benchmark = False  # å…³é—­è‡ªåŠ¨ä¼˜åŒ–

def main():
    """ä¸»ç¨‹åºé€»è¾‘"""
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = load_model_tokenizer(model_path)

    # åˆå§‹åŒ–ä¼šè¯æ¶ˆæ¯
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.chat_messages = []

    messages = st.session_state.messages

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for i, message in enumerate(messages):
        if message["role"] == "assistant":
            with st.chat_message("assistant", avatar=image_url):
                # å¤„ç†å¹¶æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯
                st.markdown(process_assistant_content(message["content"]), unsafe_allow_html=True)
                # æ·»åŠ åˆ é™¤æŒ‰é’®
                if st.button("Ã—", key=f"delete_{i}"):
                    st.session_state.messages = st.session_state.messages[:i - 1]
                    st.session_state.chat_messages = st.session_state.chat_messages[:i - 1]
                    st.rerun()
        else:
            # ç”¨æˆ·æ¶ˆæ¯å³å¯¹é½æ˜¾ç¤º
            st.markdown(
                f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: gray; border-radius: 10px; color:white; ">{message["content"]}</div></div>',
                unsafe_allow_html=True)

    # ç”¨æˆ·è¾“å…¥å¤„ç†
    prompt = st.chat_input(key="input", placeholder="ç»™ MiniMind å‘é€æ¶ˆæ¯")

    # å¤„ç†é‡æ–°ç”Ÿæˆé€»è¾‘
    if hasattr(st.session_state, 'regenerate') and st.session_state.regenerate:
        prompt = st.session_state.last_user_message
        regenerate_index = st.session_state.regenerate_index
        # æ¸…é™¤é‡æ–°ç”Ÿæˆç›¸å…³çŠ¶æ€
        delattr(st.session_state, 'regenerate')
        delattr(st.session_state, 'last_user_message')
        delattr(st.session_state, 'regenerate_index')

    if prompt:
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.markdown(
            f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px;  background-color: gray; border-radius: 10px; color:white; ">{prompt}</div></div>',
            unsafe_allow_html=True)
        # æ›´æ–°æ¶ˆæ¯è®°å½•
        messages.append({"role": "user", "content": prompt})
        st.session_state.chat_messages.append({"role": "user", "content": prompt})

        # ç”ŸæˆåŠ©æ‰‹å›å¤
        with st.chat_message("assistant", avatar=image_url):
            placeholder = st.empty()  # å ä½ç¬¦ç”¨äºæµå¼æ˜¾ç¤º
            random_seed = random.randint(0, 2**32-1)  # ç”Ÿæˆéšæœºç§å­
            setup_seed(random_seed)  # è®¾ç½®éšæœºç§å­

            # æ„é€ å†å²å¯¹è¯ä¸Šä¸‹æ–‡
            st.session_state.chat_messages = system_prompt + st.session_state.chat_messages[-(st.session_state.history_chat_num + 1):]
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            new_prompt = tokenizer.apply_chat_template(
                st.session_state.chat_messages,
                tokenize=False,
                add_generation_prompt=True
            )[-(st.session_state.max_new_tokens - 1):]  # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦

            # ç”Ÿæˆå›ç­”
            x = torch.tensor(tokenizer(new_prompt)['input_ids'], device=device).unsqueeze(0)
            with torch.no_grad():
                res_y = model.generate(
                    x, 
                    eos_token_id=tokenizer.eos_token_id,
                    max_new_tokens=st.session_state.max_new_tokens,
                    temperature=st.session_state.temperature,
                    top_p=st.session_state.top_p,
                    stream=True  # å¯ç”¨æµå¼ç”Ÿæˆ
                )
                try:
                    for y in res_y:
                        answer = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)
                        # è·³è¿‡æ— æ•ˆå­—ç¬¦
                        if (answer and answer[-1] == 'ï¿½') or not answer:
                            continue
                        # å®æ—¶æ›´æ–°æ˜¾ç¤ºå†…å®¹
                        placeholder.markdown(process_assistant_content(answer), unsafe_allow_html=True)
                except StopIteration:
                    print("No answer")

                # åå¤„ç†ç”Ÿæˆçš„å›ç­”
                assistant_answer = answer.replace(new_prompt, "")
                # æ›´æ–°æ¶ˆæ¯è®°å½•
                messages.append({"role": "assistant", "content": assistant_answer})
                st.session_state.chat_messages.append({"role": "assistant", "content": assistant_answer})

                # ä¸ºæœ€æ–°æ¶ˆæ¯æ·»åŠ åˆ é™¤æŒ‰é’®
                with st.empty():
                    if st.button("Ã—", key=f"delete_{len(messages) - 1}"):
                        st.session_state.messages = st.session_state.messages[:-2]
                        st.session_state.chat_messages = st.session_state.chat_messages[:-2]
                        st.rerun()

if __name__ == "__main__":
    from transformers import AutoModelForCausalLM, AutoTokenizer
    main()